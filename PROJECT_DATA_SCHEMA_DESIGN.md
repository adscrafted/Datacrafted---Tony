# ProjectData Schema Design

## Overview
Production-ready schema for storing compressed CSV/Excel data in Supabase PostgreSQL, optimized for datasets up to 10K rows.

---

## Schema Design

### Core Fields

#### **Primary Key & Timestamps**
```prisma
id          String    @id @default(cuid())
createdAt   DateTime  @default(now())
updatedAt   DateTime  @updatedAt
projectId   String
version     Int       @default(1)
```
- Uses CUID for distributed ID generation
- Version tracking enables data history/rollback
- Auto-updated timestamps for audit trail

#### **File Metadata** (108 bytes overhead)
```prisma
originalFileName     String
originalFileSize     Int
mimeType             String
fileHash             String?  // SHA-256 for deduplication
```
- Hash enables deduplication (prevent storing same file twice)
- Preserves original file context for user reference

#### **Compressed Data Storage**
```prisma
compressedData       Bytes
compressionAlgorithm String    @default("gzip")
uncompressedSize     Int
```
- **BYTEA column** stores compressed binary data
- **Compression ratio**: Typically 70-90% for CSV data
- **10K rows example**: 5MB uncompressed â†’ 500KB-1.5MB compressed

#### **Data Metadata** (Critical for queries)
```prisma
rowCount      Int
columnCount   Int
columnNames   String  // JSON: ["col1", "col2", ...]
columnTypes   String  // JSON: {"col1": "number", "col2": "string"}
```
- Fast queries without decompression
- Column type inference enables smart filtering

#### **Sample Data** (Quick Preview)
```prisma
sampleData    String?  // JSON: First 100 rows uncompressed
```
- Enables instant preview without full decompression
- Max 100 rows recommended (typically <100KB)

#### **Data Quality Metrics**
```prisma
nullCount           Int     @default(0)
duplicateRowCount   Int     @default(0)
dataQualityScore    Float?  // 0-100
```
- Calculate once during upload, query anytime
- Quality score formula:
  ```
  score = 100 - (nullCount/totalCells * 50) - (duplicateRows/totalRows * 50)
  ```

#### **Status Tracking**
```prisma
status     String   @default("active")  // active, archived, deleted
isActive   Boolean  @default(true)
```
- Soft delete capability (set status="deleted")
- Archive old versions without data loss

---

## Index Strategy

### Performance-Critical Indexes

```prisma
@@index([projectId])                  // Most common query
@@index([projectId, version])         // Version history queries
@@index([projectId, status])          // Filter by status
@@index([fileHash])                   // Deduplication check
@@index([createdAt])                  // Time-series queries
@@index([isActive, projectId])        // Active data queries
```

### Index Rationale

1. **[projectId]** - Clustered access pattern (80% of queries)
2. **[projectId, version]** - Composite for version history
3. **[fileHash]** - O(1) deduplication lookup
4. **[createdAt]** - Time-based analytics dashboard
5. **[isActive, projectId]** - Filter out archived/deleted efficiently

### Index Size Estimation (10K projects)
- Each B-tree index: ~100-500KB
- 6 indexes: ~3-5MB total
- Query speedup: 100-1000x vs full table scan

---

## Performance Characteristics

### Storage Efficiency

| Dataset Size | Uncompressed | Compressed (gzip) | With Indexes |
|--------------|--------------|-------------------|--------------|
| 1K rows      | 500 KB       | 50-150 KB         | +10 KB       |
| 5K rows      | 2.5 MB       | 250-750 KB        | +25 KB       |
| 10K rows     | 5 MB         | 500 KB-1.5 MB     | +50 KB       |

**Compression Algorithm Choice:**
- **gzip**: 70-90% reduction, fast decompression (recommended)
- **brotli**: 75-95% reduction, slower decompression
- **zstd**: 75-90% reduction, fastest decompression (if available)

### Query Performance

```sql
-- Fast queries (uses index, no decompression)
SELECT id, rowCount, columnCount, createdAt
FROM project_data
WHERE projectId = ? AND isActive = true;

-- Execution time: <5ms
-- Index scan: projectId + isActive
```

```sql
-- Medium queries (decompress metadata only)
SELECT id, columnNames, columnTypes, sampleData
FROM project_data
WHERE projectId = ?
ORDER BY version DESC
LIMIT 1;

-- Execution time: 10-50ms
-- Sample data is already uncompressed
```

```sql
-- Expensive queries (full decompression)
-- Should be cached or done asynchronously
SELECT compressedData FROM project_data WHERE id = ?;

-- Execution time: 50-200ms (10K rows)
-- Decompress in application layer
```

---

## Migration Guide

### Step 1: Generate Migration

```bash
npx prisma migrate dev --name add_project_data_model
```

### Step 2: Verify Migration SQL

Prisma will generate SQL similar to:
```sql
CREATE TABLE "project_data" (
    "id" TEXT NOT NULL,
    "createdAt" TIMESTAMP(3) NOT NULL DEFAULT CURRENT_TIMESTAMP,
    "updatedAt" TIMESTAMP(3) NOT NULL,
    "projectId" TEXT NOT NULL,
    "version" INTEGER NOT NULL DEFAULT 1,
    "originalFileName" TEXT NOT NULL,
    "originalFileSize" INTEGER NOT NULL,
    "mimeType" TEXT NOT NULL,
    "fileHash" TEXT,
    "compressedData" BYTEA NOT NULL,
    "compressionAlgorithm" TEXT NOT NULL DEFAULT 'gzip',
    "uncompressedSize" INTEGER NOT NULL,
    "rowCount" INTEGER NOT NULL,
    "columnCount" INTEGER NOT NULL,
    "columnNames" TEXT NOT NULL,
    "columnTypes" TEXT NOT NULL,
    "sampleData" TEXT,
    "nullCount" INTEGER NOT NULL DEFAULT 0,
    "duplicateRowCount" INTEGER NOT NULL DEFAULT 0,
    "dataQualityScore" DOUBLE PRECISION,
    "processingTimeMs" INTEGER,
    "parsingErrors" TEXT,
    "status" TEXT NOT NULL DEFAULT 'active',
    "isActive" BOOLEAN NOT NULL DEFAULT true,

    CONSTRAINT "project_data_pkey" PRIMARY KEY ("id")
);

CREATE INDEX "project_data_projectId_idx" ON "project_data"("projectId");
CREATE INDEX "project_data_projectId_version_idx" ON "project_data"("projectId", "version");
CREATE INDEX "project_data_projectId_status_idx" ON "project_data"("projectId", "status");
CREATE INDEX "project_data_fileHash_idx" ON "project_data"("fileHash");
CREATE INDEX "project_data_createdAt_idx" ON "project_data"("createdAt");
CREATE INDEX "project_data_isActive_projectId_idx" ON "project_data"("isActive", "projectId");

ALTER TABLE "project_data" ADD CONSTRAINT "project_data_projectId_fkey"
FOREIGN KEY ("projectId") REFERENCES "projects"("id") ON DELETE CASCADE ON UPDATE CASCADE;
```

### Step 3: Apply to Production

```bash
# Preview changes without applying
npx prisma migrate diff --from-migrations --to-schema-datamodel

# Apply to production Supabase
npx prisma migrate deploy
```

### Step 4: Regenerate Prisma Client

```bash
npx prisma generate
```

---

## Usage Examples

### 1. Upload and Store Data

```typescript
import { PrismaClient } from '@/lib/generated/prisma';
import { gzip } from 'zlib';
import { promisify } from 'util';

const prisma = new PrismaClient();
const gzipAsync = promisify(gzip);

async function storeProjectData(
  projectId: string,
  csvData: any[],
  fileName: string,
  fileSize: number,
  mimeType: string
) {
  const startTime = Date.now();

  // Convert to JSON and compress
  const jsonData = JSON.stringify(csvData);
  const compressedBuffer = await gzipAsync(Buffer.from(jsonData));

  // Extract metadata
  const columnNames = Object.keys(csvData[0] || {});
  const columnTypes = inferColumnTypes(csvData, columnNames);

  // Calculate quality metrics
  const nullCount = calculateNullCount(csvData);
  const duplicateRowCount = findDuplicates(csvData);
  const dataQualityScore = calculateQualityScore(
    nullCount,
    duplicateRowCount,
    csvData.length,
    columnNames.length
  );

  // Store in database
  const projectData = await prisma.projectData.create({
    data: {
      projectId,
      originalFileName: fileName,
      originalFileSize: fileSize,
      mimeType,
      fileHash: calculateHash(jsonData),
      compressedData: compressedBuffer,
      compressionAlgorithm: 'gzip',
      uncompressedSize: Buffer.byteLength(jsonData),
      rowCount: csvData.length,
      columnCount: columnNames.length,
      columnNames: JSON.stringify(columnNames),
      columnTypes: JSON.stringify(columnTypes),
      sampleData: JSON.stringify(csvData.slice(0, 100)),
      nullCount,
      duplicateRowCount,
      dataQualityScore,
      processingTimeMs: Date.now() - startTime,
      status: 'active',
      isActive: true,
    },
  });

  return projectData;
}
```

### 2. Retrieve and Decompress Data

```typescript
import { gunzip } from 'zlib';
import { promisify } from 'util';

const gunzipAsync = promisify(gunzip);

async function getProjectData(projectDataId: string) {
  // Fetch from database
  const projectData = await prisma.projectData.findUnique({
    where: { id: projectDataId },
    select: {
      compressedData: true,
      compressionAlgorithm: true,
      rowCount: true,
      columnNames: true,
    },
  });

  if (!projectData) throw new Error('Project data not found');

  // Decompress
  const decompressedBuffer = await gunzipAsync(projectData.compressedData);
  const data = JSON.parse(decompressedBuffer.toString());

  return {
    data,
    rowCount: projectData.rowCount,
    columnNames: JSON.parse(projectData.columnNames),
  };
}
```

### 3. Quick Preview (No Decompression)

```typescript
async function getProjectDataPreview(projectId: string) {
  const projectData = await prisma.projectData.findFirst({
    where: {
      projectId,
      isActive: true,
      status: 'active',
    },
    orderBy: { version: 'desc' },
    select: {
      id: true,
      rowCount: true,
      columnCount: true,
      columnNames: true,
      columnTypes: true,
      sampleData: true,
      dataQualityScore: true,
      createdAt: true,
    },
  });

  if (!projectData) return null;

  return {
    ...projectData,
    columnNames: JSON.parse(projectData.columnNames),
    columnTypes: JSON.parse(projectData.columnTypes),
    sampleData: projectData.sampleData
      ? JSON.parse(projectData.sampleData)
      : null,
  };
}
```

### 4. Check for Duplicate Upload (Deduplication)

```typescript
async function checkDuplicate(projectId: string, fileHash: string) {
  const existing = await prisma.projectData.findFirst({
    where: {
      projectId,
      fileHash,
      status: 'active',
    },
    select: {
      id: true,
      originalFileName: true,
      createdAt: true,
    },
  });

  return existing;
}
```

### 5. Archive Old Versions (Keep History)

```typescript
async function archiveOldVersions(projectId: string, keepLatest = 3) {
  const allVersions = await prisma.projectData.findMany({
    where: { projectId, status: 'active' },
    orderBy: { version: 'desc' },
    select: { id: true },
  });

  const toArchive = allVersions.slice(keepLatest);

  await prisma.projectData.updateMany({
    where: {
      id: { in: toArchive.map(v => v.id) },
    },
    data: {
      status: 'archived',
      isActive: false,
    },
  });
}
```

---

## Data Quality Helpers

```typescript
function calculateNullCount(data: any[]): number {
  return data.reduce((count, row) => {
    return count + Object.values(row).filter(v => v === null || v === '').length;
  }, 0);
}

function findDuplicates(data: any[]): number {
  const seen = new Set<string>();
  let duplicates = 0;

  for (const row of data) {
    const key = JSON.stringify(row);
    if (seen.has(key)) duplicates++;
    else seen.add(key);
  }

  return duplicates;
}

function calculateQualityScore(
  nullCount: number,
  duplicateRowCount: number,
  totalRows: number,
  totalColumns: number
): number {
  const totalCells = totalRows * totalColumns;
  const nullPenalty = (nullCount / totalCells) * 50;
  const duplicatePenalty = (duplicateRowCount / totalRows) * 50;

  return Math.max(0, 100 - nullPenalty - duplicatePenalty);
}

function inferColumnTypes(data: any[], columnNames: string[]): Record<string, string> {
  const types: Record<string, string> = {};

  for (const col of columnNames) {
    const values = data.map(row => row[col]).filter(v => v !== null && v !== '');

    if (values.every(v => !isNaN(Number(v)))) {
      types[col] = 'number';
    } else if (values.every(v => !isNaN(Date.parse(v)))) {
      types[col] = 'date';
    } else {
      types[col] = 'string';
    }
  }

  return types;
}

function calculateHash(data: string): string {
  return crypto.createHash('sha256').update(data).digest('hex');
}
```

---

## Monitoring Queries

### Storage Usage by Project

```sql
SELECT
  p.id,
  p.name,
  COUNT(pd.id) as versions,
  SUM(pd.uncompressedSize) as total_uncompressed_bytes,
  SUM(LENGTH(pd."compressedData")) as total_compressed_bytes,
  ROUND(AVG(LENGTH(pd."compressedData")::numeric / pd."uncompressedSize" * 100), 2) as avg_compression_ratio
FROM projects p
JOIN project_data pd ON p.id = pd."projectId"
WHERE pd."isActive" = true
GROUP BY p.id, p.name
ORDER BY total_compressed_bytes DESC;
```

**Expected Output (10K rows per project):**
```
id       | name        | versions | uncompressed_bytes | compressed_bytes | avg_compression_ratio
---------|-------------|----------|-------------------|------------------|----------------------
proj-123 | Sales Data  | 3        | 15,000,000        | 2,250,000        | 15.00%
```

### Data Quality Dashboard

```sql
SELECT
  p.name as project_name,
  pd."rowCount",
  pd."columnCount",
  pd."dataQualityScore",
  pd."nullCount",
  pd."duplicateRowCount",
  pd."createdAt"
FROM project_data pd
JOIN projects p ON pd."projectId" = p.id
WHERE pd."isActive" = true
  AND pd."status" = 'active'
ORDER BY pd."dataQualityScore" ASC
LIMIT 10;
```

### Processing Performance

```sql
SELECT
  AVG(pd."processingTimeMs") as avg_processing_ms,
  MAX(pd."processingTimeMs") as max_processing_ms,
  AVG(pd."rowCount") as avg_rows,
  AVG(LENGTH(pd."compressedData")) as avg_compressed_bytes
FROM project_data pd
WHERE pd."createdAt" > NOW() - INTERVAL '7 days';
```

---

## Optimization Tips

### 1. Compression Strategy
- Use gzip (level 6) for balanced speed/compression
- Consider zstd for faster decompression if available
- Don't compress sample data (quick access)

### 2. Query Patterns
- Always select only needed columns
- Use `sampleData` for previews (avoid decompression)
- Cache decompressed data in Redis (TTL: 5-15 min)

### 3. Index Maintenance
```sql
-- Rebuild indexes quarterly
REINDEX TABLE project_data;

-- Analyze for query planner
ANALYZE project_data;
```

### 4. Partitioning (Optional, for >100K rows)
```sql
-- Partition by creation date (monthly)
CREATE TABLE project_data_2025_01 PARTITION OF project_data
FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');
```

---

## Rollback Plan

If issues arise, rollback migration:

```bash
# View migration history
npx prisma migrate status

# Rollback last migration
npx prisma migrate resolve --rolled-back <migration_name>

# Manually drop table (if needed)
psql $DATABASE_URL -c "DROP TABLE project_data CASCADE;"
```

---

## Design Trade-offs

| Decision | Rationale | Alternative |
|----------|-----------|-------------|
| **BYTEA for storage** | Native PostgreSQL type, efficient for <10MB | External storage (S3) for >10MB |
| **gzip compression** | Best balance (70-90% reduction) | zstd (faster) or brotli (smaller) |
| **Sample data in TEXT** | Fast preview without decompression | Decompress on demand (slower) |
| **6 indexes** | Cover 95% of queries | Fewer indexes = slower queries |
| **Soft delete** | Preserve history, enable audit | Hard delete = can't recover |
| **Version tracking** | Data lineage, rollback capability | Overwrite = lose history |

---

## Next Steps

1. Run migration: `npx prisma migrate dev --name add_project_data_model`
2. Update Prisma client: `npx prisma generate`
3. Implement upload/retrieval utilities
4. Add Redis caching layer for decompressed data
5. Set up monitoring dashboard (storage, quality, performance)

---

## Performance Benchmarks (Expected)

| Operation | 1K rows | 5K rows | 10K rows |
|-----------|---------|---------|----------|
| Compress & Store | 50ms | 150ms | 300ms |
| Decompress & Read | 30ms | 100ms | 200ms |
| Metadata Query | 2ms | 3ms | 5ms |
| Preview Query | 10ms | 15ms | 20ms |

**Database**: Supabase PostgreSQL (shared-cpu-1x)
**Network**: <50ms latency
